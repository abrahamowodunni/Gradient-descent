# Gradient Descent and Optimization Algorithms

This repository is focused on exploring **Gradient Descent** and its popular variants like **Stochastic Gradient Descent (SGD)**, **SGD with Momentum**, and the **Adam Optimizer**. While I typically implement these using libraries like `scikit-learn` and `PyTorch`, this project is an effort to dive deeper into the inner workings and math behind these algorithms.

## ðŸš§ Work in Progress ðŸš§

This is a **work in progress** as I work on deeper implementations and visualizations from scratch to enhance my understanding of these optimization techniques.

## Current Focus
- Revisiting the core principles of **Gradient Descent**.
- Implementing and visualizing **SGD** and **Momentum** from scratch.
- Exploring how **Adam** integrates adaptive learning rates and momentum.

## Algorithms Covered

- **Gradient Descent**: Standard optimization method for minimizing loss functions.
- **Stochastic Gradient Descent (SGD)**: Faster, more dynamic updates.
- **SGD with Momentum**: Adds momentum to smooth convergence.
- **Adam Optimizer**: Combines adaptive learning rates with momentum for efficiency.

## Future Work

- Expand visualizations and extend the implementations to more complex models.
- Explore additional optimizers such as **RMSProp**.

Stay tuned for updates as I refine and deepen my understanding of these techniques.

## Contact

For any questions or suggestions, feel free to reach out:  
**YAbraham Owodunni**  
**GitHub:** [yourusername](https://github.com/abrahamowodunni)
